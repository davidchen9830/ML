{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, this my first time using TF\n",
    "\n",
    "### Goals:\n",
    "- Implement a DNN with TF\n",
    "\n",
    "##### Note that this implementation is heavily inspired from courses from Arthur Douillard: https://m2dsupsdlclass.github.io/lectures-labs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape, stddev=0.01, mean=0):\n",
    "    return tf.Variable(tf.random.normal(shape, stddev=stddev, mean=mean))\n",
    "\n",
    "def apply_activation(activation, z_n):\n",
    "    dic = {\n",
    "        \"relu\": tf.nn.relu,\n",
    "        \"sigmoid\": tf.nn.sigmoid,\n",
    "        \"softmax\": tf.nn.softmax,\n",
    "        \"tanh\": tf.math.tanh,\n",
    "        \"linear\": tf.keras.activations.linear\n",
    "        # Add more of your functions if you need\n",
    "    }\n",
    "    \n",
    "    return dic[activation](z_n) # Put your parameters if needed\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "    return np.mean(np.argmax(y_pred, axis=1) == y)\n",
    "\n",
    "\n",
    "def test_model(model, x, y):\n",
    "    dataset = gen_dataset(x, y)\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    for batch_x, batch_y in dataset:\n",
    "        preds.append(model(batch_x).numpy())\n",
    "        targets.append(batch_y.numpy())\n",
    "     \n",
    "    preds, targets = np.concatenate(preds), np.concatenate(targets)\n",
    "    return accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L_Layer_Model:\n",
    "    # We consider layer to be [input, h1, h2, h3, ..., output]\n",
    "    def __init__(self, layers, activations):\n",
    "        values = [] \n",
    "        for i in range(1, len(layers)):\n",
    "            values.append(init_weights((layers[i - 1], layers[i])))\n",
    "            values.append(init_weights([layers[i]]))\n",
    "        self.values = values\n",
    "        self.activations = activations\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        if len(self.values) == 0:\n",
    "            raise Exception(\"0 Layer Model?\")\n",
    "        z1 = tf.matmul(inputs, self.values[0]) + self.values[1]\n",
    "        a1 = apply_activation(self.activations[0], z1)\n",
    "        idx = 2\n",
    "        # Leaving the activation of the last layer for tf functions\n",
    "        for i in range(2, len(layers) - 1):\n",
    "            z1 = tf.matmul(a1, self.values[idx]) + self.values[idx + 1]\n",
    "            a1 = apply_activation(self.activations[i - 1], z1)\n",
    "            idx += 2\n",
    "        return tf.matmul(a1, self.values[idx] + self.values[idx + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test this !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image Label: 0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "sample_idx = 0\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(digits.images[sample_idx], cmap=plt.cm.gray_r,\n",
    "          interpolation='nearest')\n",
    "\n",
    "plt.title(f\"Image Label: {digits.target[sample_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "- Normalize\n",
    "- Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1527, 64), (1527,)), ((270, 64), (270,)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target,\n",
    "                                                    test_size=0.15,\n",
    "                                                   random_state=37)\n",
    "# We put a random state to have reproducible results\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Center and normalize\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(x, y, batch_size=128):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=10000, seed=42)\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers # Get optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations:  ['sigmoid']\n",
      "Without Training: 0.10216110019646366\n",
      "Epoch 0, train_acc = 0.2508, test_acc = 0.3\n",
      "Epoch 1, train_acc = 0.8585, test_acc = 0.863\n",
      "Epoch 2, train_acc = 0.9391, test_acc = 0.9481\n",
      "Epoch 3, train_acc = 0.9673, test_acc = 0.9519\n",
      "Epoch 4, train_acc = 0.9777, test_acc = 0.9556\n",
      "Epoch 5, train_acc = 0.979, test_acc = 0.9593\n",
      "Epoch 6, train_acc = 0.9849, test_acc = 0.9593\n",
      "Epoch 7, train_acc = 0.9862, test_acc = 0.9593\n",
      "Epoch 8, train_acc = 0.9895, test_acc = 0.9556\n",
      "Epoch 9, train_acc = 0.9895, test_acc = 0.9519\n",
      "Epoch 10, train_acc = 0.9908, test_acc = 0.9556\n",
      "Epoch 11, train_acc = 0.9915, test_acc = 0.963\n",
      "Epoch 12, train_acc = 0.9935, test_acc = 0.9593\n",
      "Epoch 13, train_acc = 0.9948, test_acc = 0.963\n",
      "Epoch 14, train_acc = 0.9961, test_acc = 0.963\n",
      "\n",
      "Activations:  ['relu']\n",
      "Without Training: 0.09757694826457106\n",
      "Epoch 0, train_acc = 0.4538, test_acc = 0.4407\n",
      "Epoch 1, train_acc = 0.6509, test_acc = 0.6296\n",
      "Epoch 2, train_acc = 0.9463, test_acc = 0.9222\n",
      "Epoch 3, train_acc = 0.9509, test_acc = 0.9296\n",
      "Epoch 4, train_acc = 0.9509, test_acc = 0.9593\n",
      "Epoch 5, train_acc = 0.9476, test_acc = 0.937\n",
      "Epoch 6, train_acc = 0.9561, test_acc = 0.9296\n",
      "Epoch 7, train_acc = 0.9489, test_acc = 0.9333\n",
      "Epoch 8, train_acc = 0.9705, test_acc = 0.9481\n",
      "Epoch 9, train_acc = 0.9581, test_acc = 0.937\n",
      "Epoch 10, train_acc = 0.9705, test_acc = 0.937\n",
      "Epoch 11, train_acc = 0.9653, test_acc = 0.9148\n",
      "Epoch 12, train_acc = 0.9633, test_acc = 0.9444\n",
      "Epoch 13, train_acc = 0.9764, test_acc = 0.9519\n",
      "Epoch 14, train_acc = 0.9823, test_acc = 0.9593\n",
      "\n",
      "Activations:  ['tanh']\n",
      "Without Training: 0.1899148657498363\n",
      "Epoch 0, train_acc = 0.9077, test_acc = 0.9296\n",
      "Epoch 1, train_acc = 0.9692, test_acc = 0.9444\n",
      "Epoch 2, train_acc = 0.9862, test_acc = 0.9556\n",
      "Epoch 3, train_acc = 0.9908, test_acc = 0.9593\n",
      "Epoch 4, train_acc = 0.9915, test_acc = 0.963\n",
      "Epoch 5, train_acc = 0.9954, test_acc = 0.9556\n",
      "Epoch 6, train_acc = 0.9967, test_acc = 0.9593\n",
      "Epoch 7, train_acc = 0.9967, test_acc = 0.9593\n",
      "Epoch 8, train_acc = 0.998, test_acc = 0.9667\n",
      "Epoch 9, train_acc = 0.998, test_acc = 0.9667\n",
      "Epoch 10, train_acc = 0.9987, test_acc = 0.9667\n",
      "Epoch 11, train_acc = 0.9987, test_acc = 0.9667\n",
      "Epoch 12, train_acc = 0.9987, test_acc = 0.9667\n",
      "Epoch 13, train_acc = 0.9987, test_acc = 0.9667\n",
      "Epoch 14, train_acc = 0.9993, test_acc = 0.9667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers = [X_train.shape[1], 15, 10]\n",
    "# Linear in the output because it will go into the softmax\n",
    "activations_list = [[\"sigmoid\"],\n",
    "                   [\"relu\"],\n",
    "                   [\"tanh\"]]\n",
    "# Set 15 as hidden layer size\n",
    "# 10 is because we have 10 classes (digits)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 15 # How long the model should be trained\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "# Basic optimizer, up to you, I'll use the nesterov one\n",
    "optimizer = optimizers.SGD(learning_rate = learning_rate,\n",
    "                          momentum = 0.9,\n",
    "                          nesterov=True)\n",
    "for activations in activations_list:\n",
    "    model = L_Layer_Model(layers, activations)\n",
    "    print(\"Activations: \", activations)\n",
    "    print(f\"Without Training: {test_model(model, X_train, y_train)}\")\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        train_dataset = gen_dataset(X_train, y_train, batch_size)\n",
    "\n",
    "        for batch_x, batch_y in train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(batch_x)\n",
    "                loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(batch_y, logits))\n",
    "            grads = tape.gradient(loss, model.values)\n",
    "            optimizer.apply_gradients(zip(grads, model.values),\n",
    "                                    experimental_aggregate_gradients=True)\n",
    "\n",
    "        train_accuracy = test_model(model, X_train, y_train)\n",
    "        test_accuracy = test_model(model, X_test, y_test)\n",
    "\n",
    "        print(f\"Epoch {e}, train_acc = {round(train_accuracy, 4)}, test_acc = {round(test_accuracy, 4)}\")\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
